import os
import streamlit as st
from dotenv import load_dotenv

# Importa√ß√µes dos m√≥dulos LangChain
# TextLoader: Usado para carregar dados de arquivos de texto simples.
from langchain_community.document_loaders import TextLoader
# RecursiveCharacterTextSplitter: Divide documentos grandes em peda√ßos menores (chunks).
from langchain.text_splitter import RecursiveCharacterTextSplitter
# HuggingFaceEmbeddings: Gera embeddings (representa√ß√µes vetoriais num√©ricas) de texto.
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
# Chroma: Um banco de vetores leve e integrado que armazena os embeddings.
from langchain_community.vectorstores import Chroma
# ChatPromptTemplate: Ajuda a construir prompts estruturados para modelos de chat.
from langchain_core.prompts import ChatPromptTemplate
# StrOutputParser: Extrai a string de texto da resposta do modelo de linguagem.
from langchain_core.output_parsers import StrOutputParser
# RunnablePassthrough: Permite que uma entrada seja passada diretamente para o pr√≥ximo passo.
from langchain_core.runnables import RunnablePassthrough
# ChatMistralAI: Classe para interagir com os modelos de chat da API do Mistral AI.
from langchain_mistralai.chat_models import ChatMistralAI

# --- Configura√ß√£o da P√°gina Streamlit ---
st.set_page_config(
    page_title="An√°lise de Logs de Rede com RAG e Mistral AI",
    page_icon="üîç",
    layout="centered",
    initial_sidebar_state="auto"
)

st.title("üîç An√°lise de Logs de Rede com RAG e Mistral AI")
st.markdown(
    """
    Fa√ßa uma pergunta sobre os logs abaixo e obtenha insights instant√¢neos!
    """
)

# --- Carregamento de Vari√°veis de Ambiente ---
load_dotenv()
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")

if not MISTRAL_API_KEY:
    st.error("MISTRAL_API_KEY n√£o encontrada. Crie um arquivo .env com sua chave na raiz do projeto.")
    st.stop() # Interrompe a execu√ß√£o do Streamlit se a chave n√£o estiver configurada

# --- Fun√ß√£o para Configurar o Pipeline RAG (Cacheada) ---
# O decorador st.cache_resource garante que esta fun√ß√£o seja executada apenas uma vez
# quando o aplicativo Streamlit √© iniciado, e seus resultados s√£o armazenados em cache.
# Isso evita reprocessar os logs e recarregar o modelo a cada intera√ß√£o do usu√°rio.
@st.cache_resource
def setup_rag_pipeline():
    """
    Configura e retorna a cadeia RAG completa.
    Esta fun√ß√£o √© cacheada para otimizar o desempenho.
    """
    st.spinner("Carregando e processando logs... Isso pode levar alguns segundos.")

    # 1. Carregamento dos Logs de Rede
    # Usa TextLoader para ler o arquivo de logs.
    loader = TextLoader("./logs/network_logs.txt")
    documents = loader.load()
#    st.success(f"Logs de rede carregados: {len(documents)} documento(s).")

    # 2. Divis√£o dos Documentos em Chunks
    # Divide os documentos maiores em peda√ßos menores (chunks).
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    chunks = text_splitter.split_documents(documents)
#    st.success(f"Chunks criados: {len(chunks)} peda√ßos de log.")

    # 3. Gera√ß√£o de Embeddings
    # Gera embeddings para cada chunk usando um modelo pr√©-treinado.
    embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
#    st.success("Modelo de embeddings carregado.")

    # 4. Armazenamento em um Banco de Vetores (ChromaDB)
    # Cria e popula um banco de vetores ChromaDB com os chunks e embeddings.
    # O ChromaDB √© vol√°til por padr√£o aqui, mas pode ser persistente se configurado.
    vectorstore = Chroma.from_documents(chunks, embeddings_model)
#    st.success("Embeddings armazenados no ChromaDB.")

    # 5. Configura√ß√£o do Retriever
    # Configura o retriever para buscar os 3 chunks mais relevantes.
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
#    st.success("Retriever configurado.")

    # 6. Configura√ß√£o do Modelo Mistral AI
    # Inicializa o modelo de chat do Mistral AI.
    llm = ChatMistralAI(api_key=MISTRAL_API_KEY, model="mistral-small-latest")
#    st.success("Mistral AI configurado.")

    # 7. Cria√ß√£o do Prompt Template
    # Define o formato do prompt que ser√° enviado ao Mistral AI.
    template = """
    Voc√™ √© um assistente √∫til para an√°lise de logs de rede.
    Use os seguintes trechos de logs de rede recuperados para responder √† pergunta.
    Se voc√™ n√£o souber a resposta, apenas diga que n√£o tem informa√ß√µes suficientes nos logs.
    Mantenha a resposta concisa e relevante para o contexto dos logs.

    Contexto dos Logs:
    {context}

    Pergunta: {question}

    Resposta:
    """
    prompt = ChatPromptTemplate.from_template(template)
#    st.success("Prompt template criado.")

    # 8. Constru√ß√£o da Cadeia RAG
    # A cadeia RAG orquestra o fluxo de dados: retriever -> prompt -> llm -> parser.
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
#    st.success("Cadeia RAG constru√≠da com sucesso!")
    return rag_chain

# --- Inicializa o pipeline RAG ---
# Esta chamada aciona a fun√ß√£o setup_rag_pipeline() apenas uma vez.
rag_chain = setup_rag_pipeline()

# --- Interface de Intera√ß√£o ---
st.subheader("Fa√ßa sua pergunta sobre os logs:")

# st.text_input cria uma caixa de texto para a entrada do usu√°rio.
user_query = st.text_input(
    "Digite sua pergunta aqui:",
    placeholder="Ex: Houve algum ataque DDoS recente?"
)

# st.button cria um bot√£o para enviar a pergunta.
if st.button("Analisar Logs"):
    if user_query:
        with st.spinner("Processando sua pergunta e consultando os logs..."):
            # Invoca a cadeia RAG com a pergunta do usu√°rio.
            response = rag_chain.invoke(user_query)
            st.markdown("---")
            st.subheader("Resposta do Mistral AI:")
            st.info(response) # Exibe a resposta em um bloco de informa√ß√£o
    else:
        st.warning("Por favor, digite uma pergunta para analisar.")

st.markdown("---")
st.markdown("Desenvolvido por Rodrigo Cau√£ para TCC de An√°lise de Logs de Rede com RAG e Mistral AI.")
